**High-level Approach:**

- Initial Login and Setup:

At the beginning of the program, the crawler takes in username and password as the user input 
when executing the program. Then the crawler constructs a HTTP GET and send to the server using 
a socket to receive the initial webpage. After receiving the initial webpage, the crawler 
retrieves the csrftoken and the session id hidden in thet GET response. Constructs a POST request 
with the token and the session id to login to fakebook. If the login is successful, the we start 
crawling the webpage.

- Parsing for Links and Flags:

For each response, parse it for hyperlinks. 
Add each path to the set of paths to visit, provided that the path hasn't already been visited it. 
Check each response for a secret flag. If a flag is found in the webpage, add it to the secret_flags set. 
While iterating through all the paths, check each whether or not all flags are gathered.
If we all flags are gathered, gracefully exit the program.

- Status Codes:

For each HTTP response, check the status code:
	200 - Accept the response and move on.
    301/302 - Add the new url we get from the location header to our hyperlinks.
    403/404 - Abondon the link by considering the link "visited" and move on.
    500 - Retry to get the response again, using the same path.


**Challenges:**
Performance was the biggest challenge. We tried to simplify our design and look for any places
where performance could be affected by linear time operations. We utilized data structures to
make sure that adding and retrieving values are constant times. We decided to use set instead of list
(set lookup time in python is constant). Sets also guaranteed we wouldn't be looking over 
duplicate pages. In addition, we implemented gzip encoding to compress the HTTP response so that
sending requests and receving responses times are shorter. To decompress the encoded message body, we used zlib.

Being able to recognize chunks and maintain response integrity was also a challenge.
We have a specific receive from socket function that ensures that the response contains the full message
before closing the socket. We continiously receive data and add the received data to a buffer 
until no more data can be received.

**Testing:**
Printed HTTP request and responses to make sure the request patterns/protocols are correct,
and to make sure that the responses are the expected ones.
Printed paths that weren't valid to ensure that the program is crawling valid webpages.
Used Pycharm debugger to debug exceptions thrown. Compare the printed results and the paths in the sets to ensure
that there is no loops. Timed out sockets properly and handled each status codes and exceptions appropriately.

**Contributions:**
We pair-programmed for the whole project, and therefore each team member has equally contributed to the project. 
